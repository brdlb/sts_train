# Рекомендации по параметрам обучения (Batch Size и связанные параметры)

## Проблема

При обучении с 16 параллельными окружениями обнаружена проблема: обновление весов происходит слишком часто, что не позволяет накопить достаточную статистику о новой политике перед следующим обновлением.

## Текущая ситуация (ДО изменений)

### Параметры:
- `num_envs = 16` (16 параллельных окружений/столов)
- `n_steps = 2048` (шагов сбора данных перед обновлением)
- `batch_size = 256` (размер мини-батча)
- `n_epochs = 10` (количество эпох обновления)

### Анализ:
- **Эффективный размер буфера**: 2048 × 16 = **32,768 сэмплов**
- **Шагов на окружение**: 2048 / 16 = **128 шагов** (очень мало!)
- **Количество батчей за цикл**: 32,768 / 256 = **128 батчей**
- **Обновлений градиента за цикл**: 128 × 10 = **1,280 обновлений**

### Проблемы:
1. **Слишком мало данных перед обновлением**: 128 шагов на окружение ≈ 0.1-0.2 эпизода (средняя длина эпизода ~800-1000 шагов)
2. **Слишком много эпох на маленьком наборе данных**: 10 эпох на 32K сэмплов приводит к переобучению
3. **Частые обновления**: Политика обновляется слишком часто, не успевая накопить статистику

## Рекомендуемые изменения (ПОСЛЕ)

### Новые параметры:
- `n_steps = 8192` (увеличено в 4 раза)
- `batch_size = 256` (оставлен без изменений, оптимален)
- `n_epochs = 4` (уменьшено с 10 до 4)

### Анализ новых параметров:
- **Эффективный размер буфера**: 8192 × 16 = **131,072 сэмплов** (в 4 раза больше!)
- **Шагов на окружение**: 8192 / 16 = **512 шагов** (≈0.5-1 эпизод)
- **Количество батчей за цикл**: 131,072 / 256 = **512 батчей** (в 4 раза больше)
- **Обновлений градиента за цикл**: 512 × 4 = **2,048 обновлений** (в 1.6 раза больше)

### Преимущества:
1. ✅ **Больше данных перед обновлением**: Теперь собирается ~0.5-1 полный эпизод на окружение
2. ✅ **Лучшая статистика**: Больше разнообразных ситуаций в буфере перед обновлением
3. ✅ **Меньше переобучения**: Снижение эпох с 10 до 4 предотвращает переобучение на маленьких батчах
4. ✅ **Больше обновлений**: Несмотря на меньшее количество эпох, общее количество обновлений увеличилось (2,048 vs 1,280)
5. ✅ **Более стабильное обучение**: Больше данных = более стабильные градиенты и лучшая оценка преимуществ (advantages)

## Обоснование параметров

### n_steps = 8192
- **Рекомендация**: В 2-4 раза больше, чем количество шагов в среднем эпизоде
- **Обоснование**: Позволяет собрать данные из нескольких эпизодов перед обновлением
- **Для вашего случая**: Средняя длина эпизода ~800-1000 шагов, поэтому 8192 шагов = ~8-10 эпизодов на все окружения

### batch_size = 256
- **Рекомендация**: Должен быть делителем эффективного размера буфера
- **Обоснование**: 
  - 131,072 / 256 = 512 батчей (идеально делится)
  - Оптимальный баланс между стабильностью (большие батчи) и разнообразием (не слишком большие)
- **Альтернативы**: 128 (больше обновлений, но менее стабильные градиенты) или 512 (меньше обновлений, но более стабильные)

### n_epochs = 4
- **Рекомендация**: 3-6 эпох для PPO
- **Обоснование**: 
  - С большим количеством данных (n_steps=8192) меньше эпох достаточно для эффективного обучения
  - Меньше эпох = меньше риск переобучения на собранных данных
  - PPO обычно использует 3-10 эпох, но с большими буферами достаточно 4-6

## Сравнительная таблица

| Параметр | Старое значение | Новое значение | Изменение |
|----------|----------------|----------------|-----------|
| n_steps | 2,048 | 8,192 | +300% |
| batch_size | 256 | 256 | без изменений |
| n_epochs | 10 | 4 | -60% |
| Эффективный буфер | 32,768 | 131,072 | +300% |
| Шагов на env | 128 | 512 | +300% |
| Батчей за цикл | 128 | 512 | +300% |
| Обновлений за цикл | 1,280 | 2,048 | +60% |

## Дополнительные рекомендации

### Если у вас больше/меньше окружений:

**Для 8 окружений:**
- `n_steps = 8192` (то же самое, но 1024 шага на окружение)
- `batch_size = 256` или `512` (проверьте делимость: 8192 × 8 = 65,536)
- `n_epochs = 4-6`

**Для 32 окружений:**
- `n_steps = 4096` или `8192` (256 или 512 шагов на окружение)
- `batch_size = 256` или `512`
- `n_epochs = 4-6`

### Мониторинг обучения:

Следите за следующими метриками в TensorBoard:
1. **policy_loss**: Должен стабильно уменьшаться, без резких скачков
2. **value_loss**: Должен быть относительно стабильным
3. **entropy**: Должен постепенно уменьшаться (агент становится более уверенным)
4. **clip_fraction**: Должен быть < 0.2 (показывает, что clipping эффективен)

### Если обучение нестабильно:

1. **Увеличьте n_steps** до 12288 или 16384 (еще больше данных)
2. **Уменьшите learning_rate** до 1.0e-4 (более консервативные обновления)
3. **Увеличьте clip_range** до 0.25 (больше допустимых изменений политики)
4. **Уменьшите n_epochs** до 3 (еще меньше переобучения)

### Если обучение слишком медленное:

1. **Уменьшите n_steps** до 4096 (меньше данных, но чаще обновления)
2. **Увеличьте batch_size** до 512 (меньше батчей, но быстрее)
3. **Увеличьте learning_rate** до 5.0e-4 (более агрессивные обновления)

## Заключение

Рекомендуемые изменения должны значительно улучшить стабильность обучения, позволив агенту накопить больше статистики перед каждым обновлением политики. Это особенно важно в многопользовательских играх, где разнообразие ситуаций критично для обучения.

