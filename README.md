# Perudo RL Environment

Среда для обучения ботов в игре Perudo с использованием Reinforcement Learning.

## Описание

Этот проект реализует среду для обучения агентов с подкреплением (RL) в игре Perudo (Первый) с использованием:
- **PyTorch** - фреймворк для глубокого обучения
- **Stable Baselines3** - библиотека RL алгоритмов
- **Gymnasium** - стандарт для создания игровых сред
- **TensorBoard** - мониторинг обучения

## Особенности

### Основная функциональность

- **Parameter Sharing** - одна нейросеть для всех агентов, что позволяет эффективно обучать несколько агентов одновременно
- **Agent ID в Observation** - каждый агент получает свой уникальный идентификатор (one-hot encoding) в observation, что позволяет специализировать поведение
- **Векторизованная среда** - каждый VecEnv экземпляр представляет один стол с 4 агентами, что позволяет параллельно обучать на множестве столов
- **Self-Play с Opponent Pool** - система пула оппонентов для самообучения:
  - Автоматическое сохранение снапшотов политики каждые N шагов (по умолчанию 50k)
  - Хранение последних 10-20 снапшотов
  - Взвешенный выбор оппонентов на основе winrate статистики
  - Автоматическая очистка старых снапшотов с сохранением лучших
  - Отслеживание ELO рейтинга для каждого снапшота
- **Глобальная нормализация Advantages** - advantages нормализуются по всему объединенному батчу (n_steps × num_envs × num_agents)
- Классические правила Perudo (5 костей, пакао, пальфико)
- Настраиваемое количество игроков
- Интеграция с TensorBoard для визуализации метрик

### Архитектура

- **Parameter Sharing**: Одна PPO модель для всех агентов, идентификация через agent_id в observation
- **Векторизация**: Каждый environment = 1 стол с 4 агентами, один из которых - обучающийся агент (agent_id=0), остальные - оппоненты из пула
- **Batch Processing**: Эффективный батч = n_steps × num_envs × num_agents, обеспечивающий большое количество данных для обучения

## Установка

1. Клонируйте репозиторий или создайте новый проект

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

3. Убедитесь, что у вас установлен PyTorch с поддержкой CUDA (если используете GPU):
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

## Структура проекта

```
.
├── src/
│   └── perudo/
│       ├── __init__.py
│       ├── game/              # Логика игры Perudo
│       │   ├── game_state.py  # Состояние игры
│       │   ├── rules.py       # Правила игры
│       │   ├── perudo_env.py  # Gymnasium environment
│       │   └── perudo_vec_env.py  # Векторизованная среда для self-play
│       ├── agents/            # Агенты для обучения
│       │   ├── base_agent.py  # Базовый класс агента
│       │   └── rl_agent.py    # RL агент на базе SB3
│       ├── training/          # Модули обучения
│       │   ├── config.py      # Конфигурация обучения
│       │   ├── train.py       # Главный скрипт обучения с parameter sharing
│       │   └── opponent_pool.py  # Управление пулом оппонентов
│       └── utils/             # Вспомогательные функции
│           └── helpers.py     # Утилиты (encoding/decoding, observation)
├── tests/                     # Тесты
│   ├── test_game_state.py
│   ├── test_perudo_env.py
│   ├── test_perudo_vec_env.py
│   ├── test_opponent_pool.py
│   ├── test_helpers.py
│   └── test_rules.py
├── logs/                      # Логи TensorBoard
├── models/                    # Сохраненные модели
│   └── opponent_pool/        # Снапшоты оппонентов
├── requirements.txt
└── README.md
```

## Использование

### Базовый запуск обучения

```bash
python -m src.perudo.training.train
```

### Запуск с параметрами

```bash
python -m src.perudo.training.train \
    --num-players 4 \
    --num-envs 8 \
    --total-timesteps 1000000 \
    --n-steps 2048 \
    --batch-size 64
```

### Параметры командной строки

- `--num-players` - количество игроков за столом (по умолчанию 4)
- `--num-envs` - количество параллельных сред/столов (по умолчанию 8)
- `--total-timesteps` - общее количество шагов обучения (по умолчанию 1_000_000)
- `--n-steps` - количество шагов для сбора данных перед обновлением (по умолчанию 2048)
- `--batch-size` - размер батча для обучения (по умолчанию 64)

**Важно**: Убедитесь, что `n_steps × num_envs × num_players` делится на `batch_size` без остатка.

### Программное использование

```python
from src.perudo.training.train import SelfPlayTraining
from src.perudo.training.config import DEFAULT_CONFIG

# Создать конфигурацию
config = DEFAULT_CONFIG
config.game.num_players = 4
config.training.num_envs = 8
config.training.total_timesteps = 1_000_000
config.training.n_steps = 2048
config.training.batch_size = 64

# Запустить обучение
trainer = SelfPlayTraining(config)
trainer.train()
```

### Просмотр метрик в TensorBoard

```bash
tensorboard --logdir=logs/
```

## Архитектура обучения

### Parameter Sharing

Вместо обучения отдельных моделей для каждого агента, используется одна модель PPO для всех агентов. Агенты идентифицируются через one-hot encoding agent_id в observation vector. Это позволяет:

- Эффективно использовать данные от всех агентов
- Обучать агентов с разными стратегиями (специализация через agent_id)
- Сократить использование памяти и вычислительных ресурсов

### Векторизованная среда

`PerudoMultiAgentVecEnv` создает несколько параллельных сред (столов), где каждая среда содержит 4 агентов:

- **Agent 0** - обучающийся агент (использует текущую PPO модель)
- **Agents 1-3** - оппоненты, выбираемые из opponent pool

При каждом `reset()` оппоненты выбираются из пула на основе winrate статистики.

### Opponent Pool

Opponent pool управляет снапшотами политик для self-play:

1. **Сохранение снапшотов**: Автоматически каждые N шагов (по умолчанию 50k)
2. **Хранение**: Последние 10-20 снапшотов
3. **Выбор оппонентов**: Взвешенный выбор на основе winrate (более сложные оппоненты выбираются чаще)
4. **Статистика**: Отслеживание winrate и ELO для каждого снапшота
5. **Очистка**: Автоматическое удаление старых снапшотов с сохранением лучших

### Batch Processing

Эффективный размер батча рассчитывается как:
```
effective_batch_size = n_steps × num_envs × num_players
```

Advantages нормализуются глобально по всему батчу перед обновлением модели.

## Правила игры Perudo

Perudo - это игра в кости с элементами блефа и неполной информацией. 

### Основные правила:

- Каждый игрок начинает с 5 костями
- Игроки делают ставки на общее количество костей определенного значения на всех костях
- Ставка должна быть выше предыдущей (больше количество ИЛИ такое же количество, но больше значение)
- Специальное значение "1" (pasari) считается как "двойное" значение
- Можно вызвать предыдущего игрока (challenge) - если ставка неверна, проигравший теряет кость
- Можно вызвать "пакао" (pacao) - все показывают кости, проверяется ставка
- Проигравшие теряют кости
- Когда у игрока остается 1 кость, активируется "пальфико" (palifico)
- Побеждает последний оставшийся игрок

## Тестирование

Запустить все тесты:

```bash
pytest tests/ -v
```

Запустить конкретный тест:

```bash
pytest tests/test_perudo_vec_env.py -v
```

## Конфигурация

Основные параметры обучения находятся в `src/perudo/training/config.py`:

- `n_steps`: Количество шагов для сбора данных (по умолчанию 2048)
- `batch_size`: Размер батча (по умолчанию 64)
- `learning_rate`: Скорость обучения (по умолчанию 3e-4)
- `gamma`: Коэффициент дисконтирования (по умолчанию 0.99)
- `clip_range`: Параметр clipping для PPO (по умолчанию 0.2)
- И другие стандартные параметры PPO

## Лицензия

MIT License

